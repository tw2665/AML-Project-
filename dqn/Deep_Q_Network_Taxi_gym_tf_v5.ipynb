{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Q_Network_Taxi_gym_tf_v5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-1_7UKslsDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## setup\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# clear rendered env display\n",
        "from IPython.display import clear_output\n",
        "# freeze rendered env display\n",
        "from time import sleep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_-GIyJmrjJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## hyperparameter\n",
        "\n",
        "# You should change\n",
        "# LEARNING_RATE = 0.01\n",
        "LEARNING_RATE = 0.00025\n",
        "MOMENTUM = 0.95\n",
        "ALPHA_DECAY = 0.01\n",
        "HIDDEN_NODES_1 = 48\n",
        "HIDDEN_NODES_2 = 24\n",
        "# BATCH_SIZE = 16\n",
        "BATCH_SIZE = 32\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "GAMMA = 1.0\n",
        "EPSILON = 1.0\n",
        "# EPSILON_DECAY = 0.99999\n",
        "EPSILON_DECAY = 0.99995\n",
        "# EPSILON_MIN = 0.1\n",
        "EPSILON_MIN = 0.01\n",
        "T_RANGE = 201\n",
        "STATE_SIZE = 1\n",
        "ACTION_SIZE = 6\n",
        "EPISODES = 5000\n",
        "MONITOR_INTERVAL = 100\n",
        "# EPISODES = 100\n",
        "# MONITOR_INTERVAL = 10\n",
        "\n",
        "# Don't change\n",
        "INPUT_SIZE = 1\n",
        "OUTPUT_SIZE = 6\n",
        "SEED = 123"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDiOcWs7p7y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## environment\n",
        "\n",
        "ENV_NAME = 'Taxi-v3'\n",
        "env = gym.make(ENV_NAME)\n",
        "# env.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGdzqe4FxFOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## experience replay\n",
        "\n",
        "class Replay:\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "    self.length = 0\n",
        "    self.max_length = 10000\n",
        "\n",
        "  def write(self, data):\n",
        "    if self.length >= self.max_length:\n",
        "      self.buffer.pop(0)\n",
        "      self.length -= 1\n",
        "    self.buffer.append(data)\n",
        "    self.length += 1\n",
        "\n",
        "  def read(self, batch_size):\n",
        "    # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
        "    return random.sample(self.buffer, min(batch_size, self.length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fegFGHDI7MUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## network\n",
        "\n",
        "class Network:\n",
        "  def __init__(self, n_in, n_out):\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "\n",
        "  def _build_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
        "    model.add(Dense(48, activation = 'relu'))\n",
        "    model.add(Dense(self.n_out, activation = 'linear'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "    model.compile(loss = 'mse', optimizer = optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EAplrKU7g3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## agent\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.n_in = STATE_SIZE\n",
        "    self.n_out = ACTION_SIZE\n",
        "    self.total_reward = 0\n",
        "    self.gamma = GAMMA\n",
        "    self.epsilon = EPSILON\n",
        "    self.epsilon_min = EPSILON_MIN\n",
        "    self.epsilon_decay = EPSILON_DECAY\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    self.replay_buffer = Replay()\n",
        "    self.model = Network(self.n_in, self.n_out)._build_model()\n",
        "\n",
        "  def gather_experience(self, last_observation, action, reward, observation):\n",
        "    self.replay_buffer.write((last_observation, action, reward, observation))\n",
        "\n",
        "  # return action index\n",
        "  def choose_action(self, observation):\n",
        "    # epsilon greedy policy is performed here\n",
        "    # exploitation\n",
        "    # np.random.rand is uniform [0,1]\n",
        "    if np.random.rand() > self.epsilon:\n",
        "      return np.argmax(self.model.predict(np.array([observation])))\n",
        "    # exploration\n",
        "    else:\n",
        "      # random action from 0 to 5 out of 6 actions\n",
        "      return int(np.random.randint(low = 0, high = ACTION_SIZE-1, size = 1, dtype = 'int'))\n",
        "\n",
        "  # set total reward\n",
        "  def set_total_reward(self, new_total):\n",
        "    self.total_reward = new_total\n",
        "\n",
        "  # gather reward\n",
        "  def gather_reward(self, reward):\n",
        "    self.total_reward += reward\n",
        "\n",
        "  # get total rewards\n",
        "  def get_total_reward(self):\n",
        "    return self.total_reward\n",
        "\n",
        "  # we start from large epsilon and gradually decay the epsilon in each episode\n",
        "  def decay_epsilon(self):\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  # q update\n",
        "  def q_update(self):\n",
        "    # get a batch from replay buffer\n",
        "    # batch is list of turples\n",
        "    batch = self.replay_buffer.read(self.batch_size)\n",
        "    # s[0] is last_observation\n",
        "    # first element of turple in the list is state index\n",
        "    q_last = self.model.predict(np.array([s[0] for s in batch]))\n",
        "\n",
        "    # initialize\n",
        "    q_this = np.zeros_like(q_last)\n",
        "    # batch[i][3] is observation\n",
        "    ind_not_none = [i for i in range(np.shape(batch)[0]) if batch[i][3] is not None]\n",
        "    # b[3] is observation\n",
        "    q_this_not_none = self.model.predict(np.array([b[3] for b in batch if b[3] is not None]))\n",
        "    \n",
        "    for i in range(len(ind_not_none)):\n",
        "      # store n_out number of q predictions by neural network regression\n",
        "      q_this[ind_not_none[i], :] = q_this_not_none[i, :]\n",
        "\n",
        "    x_batch = np.zeros([np.shape(batch)[0], self.n_in])\n",
        "    y_batch = np.zeros([np.shape(batch)[0], self.n_out])\n",
        "\n",
        "    for i in range(np.shape(batch)[0]):\n",
        "      # batch[i][0] is last_observation\n",
        "      x_batch[i, :] = batch[i][0]\n",
        "\n",
        "      for j in range(self.n_out):\n",
        "        # batch[i][1] is action\n",
        "        if j == batch[i][1]:\n",
        "          # batch[i][2] is reward\n",
        "          # This is y_i\n",
        "          y_batch[i, j] = batch[i][2] + self.gamma * np.max(q_this[i])\n",
        "        else:\n",
        "          y_batch[i, j] = q_last[i][j]\n",
        "\n",
        "    self.model.fit(x_batch, y_batch, epochs = 1, verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck7Vqoby8ZoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e2b712e-4139-4332-fe61-8dac747a974f"
      },
      "source": [
        "agent = Agent()\n",
        "ep_rewards = []\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "\n",
        "  # monitor training process\n",
        "  if ep % MONITOR_INTERVAL == 0:\n",
        "    print(\"episode\", ep, \"epsilon\", agent.epsilon)\n",
        "\n",
        "  # initialize\n",
        "  # env.reset() in taxi returns index of states out of 500\n",
        "  last_observation = env.reset()\n",
        "  agent.set_total_reward(0)\n",
        "\n",
        "  # iterations within an episode\n",
        "  for t in range(T_RANGE):\n",
        "\n",
        "    # draw action\n",
        "    action = agent.choose_action(last_observation)\n",
        "    # draw next state and reward\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    # when taxi drop a passenger at destination, done = True\n",
        "    if done == True:\n",
        "      observation = None\n",
        "\n",
        "    # accumulate reward\n",
        "    agent.gather_reward(reward)\n",
        "    agent.gather_experience(last_observation, action, reward, observation)\n",
        "\n",
        "    # update q function\n",
        "    agent.q_update()\n",
        "    # iterate\n",
        "    last_observation = observation\n",
        "        \n",
        "    # goal\n",
        "    if done == True:\n",
        "      ep_rewards.append(agent.get_total_reward())\n",
        "      break\n",
        "\n",
        "  # In each episode we decay epsilon\n",
        "  agent.decay_epsilon()\n",
        "\n",
        "  # Monitor total reward during episodes\n",
        "  if ep % MONITOR_INTERVAL == 0:\n",
        "    print(\"reward\", agent.get_total_reward())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0 epsilon 1.0\n",
            "reward -542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI7M1xWLUzGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "episode = range(0, EPISODES, 1)\n",
        "plt.plot(episode, ep_rewards)\n",
        "plt.ylabel(\"total rewards per episode\")\n",
        "plt.xlabel(\"episode\")\n",
        "plt.title(\"DQN Taxi-v3 q-learning (training)\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpjU5wVGM7MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## debug trainig\n",
        "\n",
        "# EPSILON = 0.1\n",
        "# agent = Agent()\n",
        "# # agent.model.predict(np.array([100]))\n",
        "# last_observation = env.reset()\n",
        "# print(last_observation)\n",
        "# action = agent.choose_action(np.array([last_observation]))\n",
        "# print(action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RdHhFF1MnT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replay_buffer = Replay()\n",
        "# replay_buffer.write"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixCm9qOuLyX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## debug neural network\n",
        "\n",
        "# class Network:\n",
        "#   def __init__(self, n_in, n_out):\n",
        "#     self.n_in = n_in\n",
        "#     self.n_out = n_out\n",
        "#     self.model = self._build_model()\n",
        "\n",
        "#   def _build_model(self):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
        "#     # model.add(Dense(24, input_dim = self.n_in, activation = 'relu'))\n",
        "#     model.add(Dense(48, activation = 'relu'))\n",
        "#     model.add(Dense(self.n_out, activation = 'linear'))\n",
        "\n",
        "#     # optimizer = tf.keras.optimizer.RMSprop(0.001)\n",
        "#     model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
        "\n",
        "#     return model\n",
        "\n",
        "# net = Network(n_in = 1, n_out = 6)\n",
        "# # net.model.summary()\n",
        "\n",
        "# x = np.array([1])\n",
        "# pred = net.model.predict(x)\n",
        "# print(pred[0])\n",
        "# print(pred)\n",
        "# print(np.argmax(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZQitciWBjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Debug replay buffer\n",
        "\n",
        "# replay_buffer = Replay()\n",
        "# last_observation = env.reset()\n",
        "\n",
        "# for _ in range(10):\n",
        "\n",
        "#   action = 0\n",
        "#   observation, reward, done, info = env.step(action)\n",
        "#   # last_observation, action, reward, observation\n",
        "#   replay_buffer.write((last_observation, action, reward, observation))\n",
        "#   last_observation = observation\n",
        "\n",
        "# print(replay_buffer.buffer)\n",
        "\n",
        "# batch = replay_buffer.read(3)\n",
        "# print(batch)\n",
        "\n",
        "# print([s[0] for s in batch])\n",
        "# print('np.shape(batch)[0]', np.shape(batch)[0])\n",
        "# batch[0][3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-IYLO68QYqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(env.observation_space)\n",
        "# print(env.action_space)\n",
        "# observation = env.reset()\n",
        "# print(\"observation\", observation)\n",
        "# print(\"env.step\", env.step(0))\n",
        "# nb_actions = env.action_space.n\n",
        "# nb_states = env.observation_space\n",
        "# print(nb_actions)\n",
        "# print(nb_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2CkqX9XHO1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reply = Replay()\n",
        "# action = 0\n",
        "# last_observation,  = env.step(action)\n",
        "# print(step)\n",
        "# reply.write()\n",
        "# reply.read(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j7rDOdWtQ9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## random exploration\n",
        "\n",
        "# for ep in range(20):\n",
        "#   observation = env.reset()\n",
        "#   for t in range(100):\n",
        "#     clear_output(wait = True)\n",
        "#     env.render()\n",
        "#     action = env.action_space.sample()\n",
        "#     observation, reward, done, info = env.step(action)\n",
        "#     sleep(.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYsu1RK32bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## demonstration\n",
        "\n",
        "# # simple demonstration that network is able to train property\n",
        "# with tf.Graph().as_default():\n",
        "#   with tf.Session() as sess:\n",
        "#     f = Network(sess, 1, 6)\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "#     # make demo input\n",
        "#     x = np.random.randn(10000, 1)\n",
        "#     # make demo output\n",
        "#     tmp_1 = 2 * x[:,0]\n",
        "#     tmp_2 = x[:,0]**2\n",
        "#     tmp_3 = x[:,0]**3\n",
        "#     tmp_4 = 0.1 * x[:,0]\n",
        "#     tmp_5 = 0.3 * x[:,0]\n",
        "#     tmp_6 = 5 * x[:,0]**2\n",
        "#     y = np.transpose([tmp_1, tmp_2, tmp_3, tmp_4, tmp_5, tmp_6])\n",
        "\n",
        "#     # check MSE before training\n",
        "#     print('MSE at iteration 0 is {}'.format(((f.compute(x) - y)**2).mean()))\n",
        "\n",
        "#     # train\n",
        "#     iteration = 5000\n",
        "#     for i in range(iteration):\n",
        "#       f.train(x, y)\n",
        "\n",
        "#     # check MSE after training\n",
        "#     print('MSE at iteration {} is {}'.format(iteration, ((f.compute(x) - y)**2).mean()))\n",
        "\n",
        "# # We can check that MSE decreased after training so our network is working"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrJF9UPR5ONP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = np.random.randn(10000, 4)\n",
        "# tmp_1 = x[:,0] + x[:,1]**2\n",
        "# tmp_2 = x[:,2] + x[:,3]**3\n",
        "# print(tmp_1.shape, tmp_2.shape)\n",
        "# y = np.transpose([ x[:,0] + x[:,1]**2, x[:,2] + x[:,3]**3 ])\n",
        "# print(x.shape, y.shape)\n",
        "\n",
        "# x = np.random.randn(10000, 1)\n",
        "# tmp_1 = 2 * x[:,0]\n",
        "# tmp_2 = x[:,0]**2\n",
        "# tmp_3 = x[:,0]**3\n",
        "# tmp_4 = 0.1 * x[:,0]\n",
        "# y = np.transpose([tmp_1, tmp_2, tmp_3, tmp_4])\n",
        "# print(x.shape, y.shape)\n",
        "# print(y[0:4, :])\n",
        "# print(y[0,:])\n",
        "# print(np.argmax(y[0, :]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvJJGAxrQDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Dense(32, input_shape = (INPUT_SIZE, ), activation = 'relu'))\n",
        "# model.add(Dense(OUTPUT_SIZE, activation = 'linear'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}