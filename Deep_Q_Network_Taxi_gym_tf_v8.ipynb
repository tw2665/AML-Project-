{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep_Q_Network_Taxi_gym_tf_v8_(onehot_improved).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0-1_7UKslsDY",
        "colab": {}
      },
      "source": [
        "## setup\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# clear rendered env display\n",
        "from IPython.display import clear_output\n",
        "# freeze rendered env display\n",
        "from time import sleep\n",
        "import time\n",
        "from google.colab import files\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_-GIyJmrjJD",
        "colab": {}
      },
      "source": [
        "## hyperparameter\n",
        "\n",
        "# You should change\n",
        "# LEARNING_RATE = 0.01\n",
        "LEARNING_RATE = 0.00025\n",
        "# MOMENTUM = 0.95\n",
        "\n",
        "# BATCH_SIZE = 16\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "GAMMA = 0.95\n",
        "# GAMMA = 1.0\n",
        "\n",
        "EPSILON = 1.0\n",
        "\n",
        "# EPSILON_DECAY = 0.99999\n",
        "# EPSILON_DECAY = 0.9995\n",
        "EPSILON_DECAY = 0.9972\n",
        "\n",
        "EPSILON_MIN = 0.1\n",
        "# EPSILON_MIN = 0.01\n",
        "\n",
        "T_RANGE = 201\n",
        "\n",
        "STATE_SIZE = 1\n",
        "ONEHOT_STATE_SIZE = 500\n",
        "\n",
        "ACTION_SIZE = 6\n",
        "\n",
        "# EPISODES = 15000\n",
        "# MONITOR_INTERVAL = 1000\n",
        "# EPISODES = 10000\n",
        "# MONITOR_INTERVAL = 100\n",
        "EPISODES = 5000\n",
        "MONITOR_INTERVAL = 100\n",
        "# EPISODES = 100\n",
        "# MONITOR_INTERVAL = 10\n",
        "# EPISODES = 10\n",
        "# MONITOR_INTERVAL = 1\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "ENV_NAME = 'Taxi-v3'\n",
        "\n",
        "PATH_SAVE_WEIGHTS = 'model.h5'\n",
        "SAVE_FIG_REWARD = 'dqn_taxi_reward.png'\n",
        "SAVE_FIG_LOSS = 'dqn_taxi_loss.png'\n",
        "REWARD_OBJ = 'reward_list.sav'\n",
        "LOSS_OBJ = 'loss_list.sav'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eDiOcWs7p7y0",
        "outputId": "1a6b3968-cdf2-43cb-dcd1-c4d3161574c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "## environment\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "# env.seed(SEED)\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[43mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGdzqe4FxFOQ",
        "colab": {}
      },
      "source": [
        "## experience replay\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self):\n",
        "        self.buffer = []\n",
        "        self.length = 0\n",
        "        self.max_length = 10000\n",
        "\n",
        "    def write(self, data):\n",
        "        if self.length >= self.max_length:\n",
        "            self.buffer.pop(0)\n",
        "            self.length -= 1\n",
        "        self.buffer.append(data)\n",
        "        self.length += 1\n",
        "\n",
        "    def read(self, batch_size):\n",
        "        # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
        "        return random.sample(self.buffer, min(batch_size, self.length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fegFGHDI7MUi",
        "colab": {}
      },
      "source": [
        "## network\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, n_in, n_out):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "\n",
        "    # 2 hidden layers model\n",
        "    def _build_model_1(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
        "        model.add(Dense(48, activation = 'relu'))\n",
        "        model.add(Dense(self.n_out, activation = 'linear'))\n",
        "        optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = optimizer)\n",
        "        return model\n",
        "    \n",
        "    # no hidden or bias layer model\n",
        "    def _build_model_2(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.n_out, input_shape = (self.n_in,), activation = 'linear', use_bias = False))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = optimizer)\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1EAplrKU7g3g",
        "colab": {}
      },
      "source": [
        "## agent\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        # choose what style you want for state representation\n",
        "        # self.n_in = STATE_SIZE\n",
        "        self.n_in = ONEHOT_STATE_SIZE\n",
        "        self.n_out = ACTION_SIZE\n",
        "        self.total_reward = 0\n",
        "        self.gamma = GAMMA\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.replay_buffer = Replay()\n",
        "        # self.model = Network(self.n_in, self.n_out)._build_model_1()\n",
        "        self.model = Network(self.n_in, self.n_out)._build_model_2()\n",
        "\n",
        "    def gather_experience(self, last_observation, action, reward, observation):\n",
        "        self.replay_buffer.write((last_observation, action, reward, observation))\n",
        "\n",
        "    # used for initilization\n",
        "    def set_total_reward(self, new_total):\n",
        "        self.total_reward = new_total\n",
        "\n",
        "    # used in each time step to accumulate reward\n",
        "    def gather_reward(self, reward):\n",
        "        self.total_reward += reward\n",
        "\n",
        "    # used at the end of episode to return total reward\n",
        "    def get_total_reward(self):\n",
        "        return self.total_reward\n",
        "\n",
        "    # we start from large epsilon and gradually decay the epsilon in each episode\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "            \n",
        "    def one_hot_encode(self, observation):\n",
        "        state = np.zeros(self.n_in)\n",
        "        state[observation] = 1\n",
        "        state = np.reshape(state, (-1, self.n_in))\n",
        "        return state\n",
        "\n",
        "    # return action index\n",
        "    def choose_action(self, observation):\n",
        "        # epsilon greedy policy\n",
        "        \n",
        "        # exploitation\n",
        "        # np.random.rand is uniform [0,1]\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            state = self.one_hot_encode(observation)\n",
        "            return np.argmax(self.model.predict(state)[0])\n",
        "        \n",
        "        # exploration\n",
        "        else:\n",
        "            # random action from 0 to 5 out of 6 actions\n",
        "            return int(np.random.randint(low = 0, high = self.n_out - 1, size = 1, dtype = 'int'))\n",
        "\n",
        "    # n_in 500 one hot encoding case\n",
        "    def q_update(self):\n",
        "        \n",
        "        # sample batch\n",
        "        batch = self.replay_buffer.read(self.batch_size)\n",
        "        n = np.shape(batch)[0]\n",
        "        \n",
        "        # initialize training data\n",
        "        x_batch = np.zeros([n, self.n_in])\n",
        "        y_batch = np.zeros([n, self.n_out])\n",
        "        \n",
        "        counter = 0\n",
        "        \n",
        "        for b in batch:\n",
        "            last_state, action, reward, state = b\n",
        "            \n",
        "            last_state = self.one_hot_encode(last_state)\n",
        "            q_last = self.model.predict(last_state)[0]\n",
        "            \n",
        "            if state is None:\n",
        "                q_last[action] = reward\n",
        "            else:\n",
        "                state = self.one_hot_encode(state)\n",
        "                q_this = self.model.predict(state)[0]\n",
        "                y = reward + self.gamma * np.max(q_this)\n",
        "                q_last[action] = y\n",
        "        \n",
        "            # store\n",
        "            x_batch[counter, :] = last_state\n",
        "            y_batch[counter, :] = q_last\n",
        "            counter += 1\n",
        "        \n",
        "        history = self.model.fit(x_batch, y_batch, epochs = 1, verbose = 0)\n",
        "        \n",
        "        # return online model loss\n",
        "        return history.history['loss'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ck7Vqoby8ZoC",
        "outputId": "1b5851fe-407e-47ce-fae0-38e94d201b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "agent = Agent()\n",
        "ep_rewards = []\n",
        "loss = False\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "\n",
        "    # initialize\n",
        "    # env.reset() in taxi returns index of states out of 500\n",
        "    last_observation = env.reset()\n",
        "    agent.set_total_reward(0)\n",
        "\n",
        "    # iterations within an episode\n",
        "    for t in range(T_RANGE):\n",
        "\n",
        "        # draw action\n",
        "        action = agent.choose_action(last_observation)\n",
        "        # draw next state and reward\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # when taxi drop a passenger at destination, done = True\n",
        "        if done == True:\n",
        "            observation = None\n",
        "\n",
        "        # accumulate reward\n",
        "        agent.gather_reward(reward)\n",
        "        agent.gather_experience(last_observation, action, reward, observation)\n",
        "\n",
        "        # update q function\n",
        "        # loss = agent.q_update()\n",
        "\n",
        "        # update q function version 2\n",
        "        if np.random.random() < 0.3 and len(ep_rewards) > 0:\n",
        "            if ep_rewards[-1] < 9.7:\n",
        "                loss = agent.q_update()\n",
        "        \n",
        "        # iterate\n",
        "        last_observation = observation\n",
        "        \n",
        "        # goal\n",
        "        if done == True:\n",
        "            ep_rewards.append(agent.get_total_reward())\n",
        "            break\n",
        "\n",
        "    # store last loss of online model\n",
        "    if loss:\n",
        "        losses.append(loss)\n",
        "\n",
        "    # In each episode we decay epsilon\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    # Monitor total reward during episodes\n",
        "    if ep % MONITOR_INTERVAL == 0 and loss:\n",
        "        print(\"episode:\", ep,\n",
        "              \"reward:\", agent.get_total_reward(),\n",
        "              \"loss:\", np.round(loss, decimals = 3), \n",
        "              \"epsilon:\", np.round(agent.epsilon, decimals = 5),\n",
        "              \"time: {} seconds\".format(np.round(time.time() - start_time, decimals = 0)))\n",
        "\n",
        "\n",
        "# when training finishes\n",
        "\n",
        "# save weights of neural network\n",
        "agent.model.save(PATH_SAVE_WEIGHTS)\n",
        "\n",
        "# plot the reward result\n",
        "episode = range(0, EPISODES, 1)\n",
        "plt.plot(episode, ep_rewards)\n",
        "plt.ylabel(\"total rewards per episode\")\n",
        "plt.xlabel(\"episode\")\n",
        "plt.title(\"DQN Taxi rewards\")\n",
        "plt.savefig(SAVE_FIG_REWARD)\n",
        "plt.show()\n",
        "\n",
        "# plot the loss result\n",
        "episode = range(0, losses, 1)\n",
        "plt.plot(episode, losses)\n",
        "plt.ylabel(\"loss per episode\")\n",
        "plt.xlabel(\"episode\")\n",
        "plt.title(\"DQN Taxi rewards\")\n",
        "plt.savefig(SAVE_FIG_LOSS)\n",
        "plt.show()\n",
        "\n",
        "# save object\n",
        "pickle.dump(ep_rewards, open(REWARD_OBJ, 'wb'))\n",
        "pickle.dump(losses, open(LOSS_OBJ, 'wb'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 1 reward: -596 loss: 2.702 epsilon: 0.99441 time: 11.0 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-41ca40d86abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mep_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m9.7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# iterate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a920c13c3b39>\u001b[0m in \u001b[0;36mq_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mq_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_this\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mq_last\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m       \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples_or_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mbatch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mmake_batches\u001b[0;34m(size, batch_size)\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtuples\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m   \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m   return [(i * batch_size, min(size, (i + 1) * batch_size))\n\u001b[1;32m    493\u001b[0m           for i in range(0, num_batches)]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZQ4uY8jjHea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(PATH_SAVE_WEIGHTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEDk_tVqjcJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(SAVE_FIG_REWARD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq4-3vpRkZ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(SAVE_FIG_LOSS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO59silnoVDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(REWARD_OBJ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flTvA3meoVJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(LOSS_OBJ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rI7M1xWLUzGm",
        "colab": {}
      },
      "source": [
        "# ## Evaluation\n",
        "\n",
        "# episode = range(0, EPISODES, 1)\n",
        "# plt.plot(episode, ep_rewards)\n",
        "# plt.ylabel(\"total rewards per episode\")\n",
        "# plt.xlabel(\"episode\")\n",
        "# plt.title(\"DQN Taxi-v3 q-learning (training)\")\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoVar_yafX-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # easier version of q update\n",
        "\n",
        "# experiences = []\n",
        "# random.seed(1)\n",
        "# # (last_observation, action, reward, observation)\n",
        "# experiences.append((1, 0, -1, 2))\n",
        "# experiences.append((2, 1, -1, 3))\n",
        "# experiences.append((3, 2, -1, 4))\n",
        "# # print(experiences)\n",
        "\n",
        "# BATCH_SIZE = 2\n",
        "# GAMMA = 0.95\n",
        "\n",
        "# net = Network(500, 6)\n",
        "# model = net._build_model_2()\n",
        "\n",
        "# def one_hot_encode(observation):\n",
        "#     state = np.zeros(500)\n",
        "#     state[observation] = 1\n",
        "#     state = np.reshape(state, (-1, 500))\n",
        "#     return state\n",
        "\n",
        "# # def q_update(self):\n",
        "# def q_update():\n",
        "#     # sample batch\n",
        "#     batch = random.sample(experiences, BATCH_SIZE)\n",
        "#     # print(batch)\n",
        "    \n",
        "#     n = np.shape(batch)[0]\n",
        "    \n",
        "#     x_batch = np.zeros([n, 500])\n",
        "#     y_batch = np.zeros([n, 6])\n",
        "    \n",
        "#     counter = 0\n",
        "    \n",
        "#     for b in batch:\n",
        "#         last_state, action, reward, state = b\n",
        "        \n",
        "#         last_state = one_hot_encode(last_state)\n",
        "#         # print(last_state.shape)\n",
        "#         # print(last_state)\n",
        "        \n",
        "#         # [0] for list of list\n",
        "#         q_last = model.predict(last_state)[0]\n",
        "#         # print(q_last)\n",
        "        \n",
        "#         if state is None:\n",
        "#             q_last[action] = reward\n",
        "#         else:\n",
        "#             state = one_hot_encode(state)\n",
        "#             # print(\"state\", state)\n",
        "#             # [0] for list of list\n",
        "#             q_this = model.predict(state)[0]\n",
        "#             # print(\"q_this\", q_this)\n",
        "#             # new_action = np.argmax(q_this)\n",
        "#             # print(\"new_action\", new_action)\n",
        "            \n",
        "#             # calculate new q\n",
        "#             # Bellman equation\n",
        "#             # y = reward + GAMMA * q_this[new_action]\n",
        "#             y = reward + GAMMA * np.max(q_this)\n",
        "#             # print(\"y\", y)\n",
        "            \n",
        "#             # updat q only for current action\n",
        "#             q_last[action] = y\n",
        "#             # print(q_last)\n",
        "        \n",
        "#         y_batch[counter, :] = q_last\n",
        "#         x_batch[counter, :] = last_state\n",
        "        \n",
        "#         counter += 1\n",
        "        \n",
        "#     # debug\n",
        "#     # print(\"x_batch.shape\", x_batch.shape)\n",
        "#     # print(\"y_batch.shape\", y_batch.shape)\n",
        "#     # print(\"x_batch\", x_batch)\n",
        "#     # print(\"y_batch\", y_batch)\n",
        "                \n",
        "#     model.fit(x_batch, y_batch, epochs = 1, verbose = 0)\n",
        "    \n",
        "# q_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gpjU5wVGM7MH",
        "colab": {}
      },
      "source": [
        "## debug trainig\n",
        "\n",
        "# EPSILON = 0.1\n",
        "# agent = Agent()\n",
        "# # agent.model.predict(np.array([100]))\n",
        "# last_observation = env.reset()\n",
        "# print(last_observation)\n",
        "# action = agent.choose_action(np.array([last_observation]))\n",
        "# print(action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-RdHhFF1MnT3",
        "colab": {}
      },
      "source": [
        "# replay_buffer = Replay()\n",
        "# replay_buffer.write"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ixCm9qOuLyX1",
        "colab": {}
      },
      "source": [
        "## debug neural network\n",
        "\n",
        "# class Network:\n",
        "#   def __init__(self, n_in, n_out):\n",
        "#     self.n_in = n_in\n",
        "#     self.n_out = n_out\n",
        "#     self.model = self._build_model()\n",
        "\n",
        "#   def _build_model(self):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
        "#     # model.add(Dense(24, input_dim = self.n_in, activation = 'relu'))\n",
        "#     model.add(Dense(48, activation = 'relu'))\n",
        "#     model.add(Dense(self.n_out, activation = 'linear'))\n",
        "\n",
        "#     # optimizer = tf.keras.optimizer.RMSprop(0.001)\n",
        "#     model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
        "\n",
        "#     return model\n",
        "\n",
        "# net = Network(n_in = 1, n_out = 6)\n",
        "# # net.model.summary()\n",
        "\n",
        "# x = np.array([1])\n",
        "# pred = net.model.predict(x)\n",
        "# print(pred[0])\n",
        "# print(pred)\n",
        "# print(np.argmax(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QrZQitciWBjt",
        "colab": {}
      },
      "source": [
        "## Debug replay buffer\n",
        "\n",
        "# replay_buffer = Replay()\n",
        "# last_observation = env.reset()\n",
        "\n",
        "# for _ in range(10):\n",
        "\n",
        "#   action = 0\n",
        "#   observation, reward, done, info = env.step(action)\n",
        "#   # last_observation, action, reward, observation\n",
        "#   replay_buffer.write((last_observation, action, reward, observation))\n",
        "#   last_observation = observation\n",
        "\n",
        "# print(replay_buffer.buffer)\n",
        "\n",
        "# batch = replay_buffer.read(3)\n",
        "# print(batch)\n",
        "\n",
        "# print([s[0] for s in batch])\n",
        "# print('np.shape(batch)[0]', np.shape(batch)[0])\n",
        "# batch[0][3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-IYLO68QYqv",
        "colab": {}
      },
      "source": [
        "# print(env.observation_space)\n",
        "# print(env.action_space)\n",
        "# observation = env.reset()\n",
        "# print(\"observation\", observation)\n",
        "# print(\"env.step\", env.step(0))\n",
        "# nb_actions = env.action_space.n\n",
        "# nb_states = env.observation_space\n",
        "# print(nb_actions)\n",
        "# print(nb_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2CkqX9XHO1-",
        "colab": {}
      },
      "source": [
        "# reply = Replay()\n",
        "# action = 0\n",
        "# last_observation,  = env.step(action)\n",
        "# print(step)\n",
        "# reply.write()\n",
        "# reply.read(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9j7rDOdWtQ9q",
        "colab": {}
      },
      "source": [
        "## random exploration\n",
        "\n",
        "# for ep in range(20):\n",
        "#   observation = env.reset()\n",
        "#   for t in range(100):\n",
        "#     clear_output(wait = True)\n",
        "#     env.render()\n",
        "#     action = env.action_space.sample()\n",
        "#     observation, reward, done, info = env.step(action)\n",
        "#     sleep(.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EJYsu1RK32bK",
        "colab": {}
      },
      "source": [
        "## demonstration\n",
        "\n",
        "# # simple demonstration that network is able to train property\n",
        "# with tf.Graph().as_default():\n",
        "#   with tf.Session() as sess:\n",
        "#     f = Network(sess, 1, 6)\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "#     # make demo input\n",
        "#     x = np.random.randn(10000, 1)\n",
        "#     # make demo output\n",
        "#     tmp_1 = 2 * x[:,0]\n",
        "#     tmp_2 = x[:,0]**2\n",
        "#     tmp_3 = x[:,0]**3\n",
        "#     tmp_4 = 0.1 * x[:,0]\n",
        "#     tmp_5 = 0.3 * x[:,0]\n",
        "#     tmp_6 = 5 * x[:,0]**2\n",
        "#     y = np.transpose([tmp_1, tmp_2, tmp_3, tmp_4, tmp_5, tmp_6])\n",
        "\n",
        "#     # check MSE before training\n",
        "#     print('MSE at iteration 0 is {}'.format(((f.compute(x) - y)**2).mean()))\n",
        "\n",
        "#     # train\n",
        "#     iteration = 5000\n",
        "#     for i in range(iteration):\n",
        "#       f.train(x, y)\n",
        "\n",
        "#     # check MSE after training\n",
        "#     print('MSE at iteration {} is {}'.format(iteration, ((f.compute(x) - y)**2).mean()))\n",
        "\n",
        "# # We can check that MSE decreased after training so our network is working"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PrJF9UPR5ONP",
        "colab": {}
      },
      "source": [
        "# x = np.random.randn(10000, 4)\n",
        "# tmp_1 = x[:,0] + x[:,1]**2\n",
        "# tmp_2 = x[:,2] + x[:,3]**3\n",
        "# print(tmp_1.shape, tmp_2.shape)\n",
        "# y = np.transpose([ x[:,0] + x[:,1]**2, x[:,2] + x[:,3]**3 ])\n",
        "# print(x.shape, y.shape)\n",
        "\n",
        "# x = np.random.randn(10000, 1)\n",
        "# tmp_1 = 2 * x[:,0]\n",
        "# tmp_2 = x[:,0]**2\n",
        "# tmp_3 = x[:,0]**3\n",
        "# tmp_4 = 0.1 * x[:,0]\n",
        "# y = np.transpose([tmp_1, tmp_2, tmp_3, tmp_4])\n",
        "# print(x.shape, y.shape)\n",
        "# print(y[0:4, :])\n",
        "# print(y[0,:])\n",
        "# print(np.argmax(y[0, :]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kHvJJGAxrQDG",
        "colab": {}
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Dense(32, input_shape = (INPUT_SIZE, ), activation = 'relu'))\n",
        "# model.add(Dense(OUTPUT_SIZE, activation = 'linear'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}