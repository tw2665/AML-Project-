{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Network for Taxi-v2 environment\n",
    "We use tf-agents to train Double Deep Q Network for Taxi-v2 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: apt-get: command not found\n",
      "Requirement already satisfied: gym==0.10.11 in /anaconda3/lib/python3.7/site-packages (0.10.11)\n",
      "Requirement already satisfied: requests>=2.0 in /anaconda3/lib/python3.7/site-packages (from gym==0.10.11) (2.21.0)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from gym==0.10.11) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /anaconda3/lib/python3.7/site-packages (from gym==0.10.11) (1.17.3)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /anaconda3/lib/python3.7/site-packages (from gym==0.10.11) (1.3.2)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from gym==0.10.11) (1.2.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (1.24.1)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyglet>=1.2.0->gym==0.10.11) (0.17.1)\n",
      "Collecting imageio==2.4.0\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from imageio==2.4.0) (1.17.3)\n",
      "Requirement already satisfied: pillow in /anaconda3/lib/python3.7/site-packages (from imageio==2.4.0) (5.4.1)\n",
      "Installing collected packages: imageio\n",
      "  Found existing installation: imageio 2.5.0\n",
      "\u001b[31mCannot uninstall 'imageio'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "Requirement already satisfied: PILLOW in /anaconda3/lib/python3.7/site-packages (5.4.1)\n",
      "Requirement already satisfied: pyglet==1.3.2 in /anaconda3/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyglet==1.3.2) (0.17.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in /anaconda3/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: EasyProcess in /anaconda3/lib/python3.7/site-packages (from pyvirtualdisplay) (0.2.7)\n",
      "Requirement already satisfied: tf-agents-nightly in /anaconda3/lib/python3.7/site-packages (0.2.0.dev20191201)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (0.1.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (1.17.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (1.12.0)\n",
      "Requirement already satisfied: tfp-nightly in /anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (0.9.0.dev20191201)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /anaconda3/lib/python3.7/site-packages (from tf-agents-nightly) (0.7.0)\n",
      "Requirement already satisfied: gast>=0.2 in /anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (0.2.2)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (4.3.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /anaconda3/lib/python3.7/site-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install xvfb\n",
    "!pip install 'gym==0.10.11'\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install PILLOW\n",
    "!pip install 'pyglet==1.3.2'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents-nightly\n",
    "try:\n",
    "  %%tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "#display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x103cef198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'Taxi-v2'\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='observation', minimum=0, maximum=499)\n",
      "Reward Spec: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=5)\n",
      "Time step: TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array(254))\n",
      "Next time step: TimeStep(step_type=array(1, dtype=int32), reward=array(-1., dtype=float32), discount=array(1., dtype=float32), observation=array(154))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:', env.time_step_spec().observation)\n",
    "print('Reward Spec:', env.time_step_spec().reward)\n",
    "print('Action Spec:', env.action_spec())\n",
    "\n",
    "time_step = env.reset()\n",
    "print('Time step:', time_step)\n",
    "# When you input integers as action into step argument, you can get the next state\n",
    "action = 1\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:', next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is python environment\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "# But to run tf-agents, we need to convert to tensorflow environment\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "When we want to build neural network, we use tf_agents.networks.q_network.QNetwork.\n",
    "<br>\n",
    "When we want to build double Deep Q Network, we use tf_agents.agents.dqn.dqn_agent.DdqnAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 layer 100 nodes\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "# The below only set 1 hidden layer, which is 100 nodes densely connected layer\n",
    "# adding other arguments to build neural network\n",
    "q_net = q_network.QNetwork(\n",
    "    # input_tensor_spec (DQN input is state)\n",
    "    train_env.observation_spec(),\n",
    "    # action_spec (DQN output is action)\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DdqnAgent(\n",
    "    # time_step_spec (something containing state and reward)\n",
    "    train_env.time_step_spec(),\n",
    "    # action_spec\n",
    "    train_env.action_spec(),\n",
    "    # q_network should be developed in advance, No default value\n",
    "    q_network=q_net,\n",
    "    # optimizer does not have default value\n",
    "    optimizer=optimizer,\n",
    "    # td_errors_loss_fn, default is element_wise_huber_loss, this example plays around this loss function\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    # counter for training\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(499)))\n",
      "agent._epsilon_greedy 0.1\n",
      "agent._optimizer <tensorflow.python.training.adam.AdamOptimizer object at 0x1a2a4d5358>\n",
      "agent._initialize <bound method DqnAgent._initialize of <tf_agents.agents.dqn.dqn_agent.DdqnAgent object at 0x1a2a4d57b8>>\n"
     ]
    }
   ],
   "source": [
    "print(train_env.time_step_spec())\n",
    "print(\"agent._epsilon_greedy\", agent._epsilon_greedy)\n",
    "print(\"agent._optimizer\", agent._optimizer)\n",
    "print(\"agent._initialize\", agent._initialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent.policy <tf_agents.policies.greedy_policy.GreedyPolicy object at 0x1a2a586c18>\n",
      "agent.collect_policy <tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x1a2a5862b0>\n"
     ]
    }
   ],
   "source": [
    "print(\"agent.policy\", agent.policy)\n",
    "print(\"agent.collect_policy\", agent.collect_policy)\n",
    "# So the default setting of collect_policy is epsilon greedy policy, so use exploration and exploitation\n",
    "# But for evaluation, we already have optimal policy, we only use greedy policy to follow our optimal policy\n",
    "# In evaluation, we don't need to explore by epsilon because it is not training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "# RandomTFPolicy has _uniform_probability function inside\n",
    "# By using maximum action index minus minimum action index, it returns uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      # Do not use the below. It use tensor objects which cause error\n",
    "      # time_step = environment.step(action_step.action)\n",
    "      # For step argument, we put numpy object as action integer like below\n",
    "      time_step = eval_env.step(action_step.action.numpy()[0])\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-794.9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(499)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(5)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))\n",
      "('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n"
     ]
    }
   ],
   "source": [
    "print(agent.collect_data_spec)\n",
    "print(agent.collect_data_spec._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  # Do not use the below. It use tensor objects which cause error\n",
    "  # next_time_step = environment.step(action_step.action)\n",
    "  # For step argument, we put numpy object as action integer like below\n",
    "  next_time_step = environment.step(action_step.action.numpy()[0])\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (Trajectory(step_type=(64, 2), observation=(64, 2), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int64, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.IteratorV2 object at 0x1a2aa05a20>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "The below program performs training with the number of num_iterations training, which is set in the Hyperparameter section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 213795176448.0\n",
      "step = 400: loss = 18826885120.0\n",
      "step = 600: loss = 124545384448.0\n",
      "step = 800: loss = 81646362624.0\n",
      "step = 1000: loss = 187360854016.0\n",
      "step = 1000: Average Return = -200.0\n",
      "step = 1200: loss = 159772934144.0\n",
      "step = 1400: loss = 14532604928.0\n",
      "step = 1600: loss = 295750303744.0\n",
      "step = 1800: loss = 77805101056.0\n",
      "step = 2000: loss = 93568884736.0\n",
      "step = 2000: Average Return = -200.0\n",
      "step = 2200: loss = 324784685056.0\n",
      "step = 2400: loss = 202084777984.0\n",
      "step = 2600: loss = 398303887360.0\n",
      "step = 2800: loss = 425610510336.0\n",
      "step = 3000: loss = 178233180160.0\n",
      "step = 3000: Average Return = -200.0\n",
      "step = 3200: loss = 51315331072.0\n",
      "step = 3400: loss = 331902812160.0\n",
      "step = 3600: loss = 529813635072.0\n",
      "step = 3800: loss = 202497261568.0\n",
      "step = 4000: loss = 545513177088.0\n",
      "step = 4000: Average Return = -200.0\n",
      "step = 4200: loss = 12434911232.0\n",
      "step = 4400: loss = 576582057984.0\n",
      "step = 4600: loss = 224522338304.0\n",
      "step = 4800: loss = 230520799232.0\n",
      "step = 5000: loss = 335274147840.0\n",
      "step = 5000: Average Return = -200.0\n",
      "step = 5200: loss = 222643896320.0\n",
      "step = 5400: loss = 113603330048.0\n",
      "step = 5600: loss = 481939947520.0\n",
      "step = 5800: loss = 490492624896.0\n",
      "step = 6000: loss = 460633997312.0\n",
      "step = 6000: Average Return = -200.0\n",
      "step = 6200: loss = 516194304000.0\n",
      "step = 6400: loss = 290818621440.0\n",
      "step = 6600: loss = 111794405376.0\n",
      "step = 6800: loss = 630121496576.0\n",
      "step = 7000: loss = 1056422821888.0\n",
      "step = 7000: Average Return = -200.0\n",
      "step = 7200: loss = 379174191104.0\n",
      "step = 7400: loss = 56935370752.0\n",
      "step = 7600: loss = 780470583296.0\n",
      "step = 7800: loss = 699260600320.0\n",
      "step = 8000: loss = 615127842816.0\n",
      "step = 8000: Average Return = -200.0\n",
      "step = 8200: loss = 699024277504.0\n",
      "step = 8400: loss = 797689184256.0\n",
      "step = 8600: loss = 1043789119488.0\n",
      "step = 8800: loss = 465649532928.0\n",
      "step = 9000: loss = 826578108416.0\n",
      "step = 9000: Average Return = -200.0\n",
      "step = 9200: loss = 548727259136.0\n",
      "step = 9400: loss = 726160048128.0\n",
      "step = 9600: loss = 580902322176.0\n",
      "step = 9800: loss = 502254075904.0\n",
      "step = 10000: loss = 141725892608.0\n",
      "step = 10000: Average Return = -200.0\n",
      "step = 10200: loss = 1259439390720.0\n",
      "step = 10400: loss = 220852748288.0\n",
      "step = 10600: loss = 1976148164608.0\n",
      "step = 10800: loss = 1225466445824.0\n",
      "step = 11000: loss = 1673555083264.0\n",
      "step = 11000: Average Return = -200.0\n",
      "step = 11200: loss = 519819362304.0\n",
      "step = 11400: loss = 627540688896.0\n",
      "step = 11600: loss = 1375820840960.0\n",
      "step = 11800: loss = 1045106262016.0\n",
      "step = 12000: loss = 683310972928.0\n",
      "step = 12000: Average Return = -200.0\n",
      "step = 12200: loss = 261631213568.0\n",
      "step = 12400: loss = 728919048192.0\n",
      "step = 12600: loss = 740323622912.0\n",
      "step = 12800: loss = 1521145348096.0\n",
      "step = 13000: loss = 849728438272.0\n",
      "step = 13000: Average Return = -200.0\n",
      "step = 13200: loss = 1761008943104.0\n",
      "step = 13400: loss = 1055330336768.0\n",
      "step = 13600: loss = 2353804083200.0\n",
      "step = 13800: loss = 297593536512.0\n",
      "step = 14000: loss = 1262642397184.0\n",
      "step = 14000: Average Return = -200.0\n",
      "step = 14200: loss = 1484120129536.0\n",
      "step = 14400: loss = 1942582984704.0\n",
      "step = 14600: loss = 1608053424128.0\n",
      "step = 14800: loss = 2133482405888.0\n",
      "step = 15000: loss = 1329311121408.0\n",
      "step = 15000: Average Return = -200.0\n",
      "step = 15200: loss = 897373044736.0\n",
      "step = 15400: loss = 1938453430272.0\n",
      "step = 15600: loss = 1012879720448.0\n",
      "step = 15800: loss = 204190613504.0\n",
      "step = 16000: loss = 855216488448.0\n",
      "step = 16000: Average Return = -200.0\n",
      "step = 16200: loss = 1475262939136.0\n",
      "step = 16400: loss = 926197612544.0\n",
      "step = 16600: loss = 252248489984.0\n",
      "step = 16800: loss = 3242531225600.0\n",
      "step = 17000: loss = 1509164843008.0\n",
      "step = 17000: Average Return = -200.0\n",
      "step = 17200: loss = 909340377088.0\n",
      "step = 17400: loss = 1825108656128.0\n",
      "step = 17600: loss = 1991337836544.0\n",
      "step = 17800: loss = 2882918678528.0\n",
      "step = 18000: loss = 1302595895296.0\n",
      "step = 18000: Average Return = -200.0\n",
      "step = 18200: loss = 2018658746368.0\n",
      "step = 18400: loss = 1312669827072.0\n",
      "step = 18600: loss = 3815464501248.0\n",
      "step = 18800: loss = 1258146103296.0\n",
      "step = 19000: loss = 630933946368.0\n",
      "step = 19000: Average Return = -200.0\n",
      "step = 19200: loss = 1270979756032.0\n",
      "step = 19400: loss = 525258326016.0\n",
      "step = 19600: loss = 1575591084032.0\n",
      "step = 19800: loss = 3020288950272.0\n",
      "step = 20000: loss = 2862080327680.0\n",
      "step = 20000: Average Return = -200.0\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "#%%time\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x103cef198>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.greedy_policy.GreedyPolicy at 0x1a2a586c18>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
