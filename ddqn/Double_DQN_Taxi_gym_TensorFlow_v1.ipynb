{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Double_DQN_Taxi_gym_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PRL9Tz_lOTo",
        "colab_type": "text"
      },
      "source": [
        "# Double DQN (DDQN) Taxi with gym and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TnB7RoilOTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## setup\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MduHh6slOTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## hyperparameter\n",
        "\n",
        "ENV_NAME = 'Taxi-v3'\n",
        "\n",
        "# seed for initial position in environment\n",
        "SEED = 123\n",
        "\n",
        "# the learning rate used by RMSProp in \"human-level control through deep reinforcement learning\"\n",
        "LEARNING_RATE = 0.00025\n",
        "MOMENTUM = 0.95\n",
        "\n",
        "# taxi environment\n",
        "STATE_SIZE = 1\n",
        "ACTION_SIZE = 6\n",
        "\n",
        "# soft target update\n",
        "# value used in \"continuous control with deep reinforcement learning\"\n",
        "TAU = 0.001 \n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# gamma in Bellman equation\n",
        "GAMMA = 0.99\n",
        "\n",
        "# epsilon in epsilon greedy algorithm\n",
        "# we implement epsilon decay\n",
        "EPSILON = 1.0\n",
        "# EPSILON_DECAY = 0.99999\n",
        "EPSILON_DECAY = 0.99995\n",
        "# EPSILON_MIN = 0.1\n",
        "EPSILON_MIN = 0.01\n",
        "\n",
        "# max step in each episode\n",
        "T_RANGE = 201\n",
        "\n",
        "# taxi environment\n",
        "STATE_SIZE = 1\n",
        "ACTION_SIZE = 6\n",
        "\n",
        "# training\n",
        "# EPISODES = 5000\n",
        "# MONITOR_INTERVAL = 100\n",
        "EPISODES = 100\n",
        "# MONITOR_INTERVAL = 10\n",
        "MONITOR_INTERVAL = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz71x8zJlOT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9a5b1ad0-b678-4bca-81d1-a71deafd259d"
      },
      "source": [
        "## environment\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "# env.seed(SEED)\n",
        "env.render()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_TfG16alOT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## experience replay\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self):\n",
        "        self.buffer = []\n",
        "        self.length = 0\n",
        "        self.max_length = 10000\n",
        "\n",
        "    def write(self, data):\n",
        "        if self.length >= self.max_length:\n",
        "            self.buffer.pop(0)\n",
        "            self.length -= 1\n",
        "        self.buffer.append(data)\n",
        "        self.length += 1\n",
        "\n",
        "    def read(self, batch_size):\n",
        "        # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
        "        return random.sample(self.buffer, min(batch_size, self.length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVlD1YllOT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## network\n",
        "\n",
        "# we use the same architectures for online_network and target_network\n",
        "# we _build_model 2 times\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, n_in, n_out):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
        "        model.add(Dense(48, activation = 'relu'))\n",
        "        model.add(Dense(self.n_out, activation = 'linear'))\n",
        "\n",
        "        optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = optimizer)\n",
        "        \n",
        "        # debug\n",
        "        # print(\"compiled model\")\n",
        "        \n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eLak-3tlOT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## agent\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.n_in = STATE_SIZE\n",
        "        self.n_out = ACTION_SIZE\n",
        "        self.total_reward = 0\n",
        "        self.gamma = GAMMA\n",
        "        self.tau = TAU\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.replay_buffer = Replay()\n",
        "        self.online_model = Network(self.n_in, self.n_out)._build_model()\n",
        "        self.target_model = Network(self.n_in, self.n_out)._build_model()\n",
        "\n",
        "    def gather_experience(self, last_observation, action, reward, observation):\n",
        "        self.replay_buffer.write((last_observation, action, reward, observation))\n",
        "\n",
        "    # return action index\n",
        "    def choose_action(self, observation):\n",
        "        # epsilon greedy policy is performed here\n",
        "        # exploitation\n",
        "        # np.random.rand is uniform [0,1]\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            observation = self.reshape_state(observation)\n",
        "            return np.argmax(self.online_model.predict(observation))\n",
        "            \n",
        "        # exploration\n",
        "        else:\n",
        "            # random action from 0 to 5 out of 6 actions\n",
        "            return int(np.random.randint(low = 0, high = ACTION_SIZE-1, size = 1, dtype = 'int'))\n",
        "\n",
        "    # set total reward\n",
        "    def set_total_reward(self, new_total):\n",
        "        self.total_reward = new_total\n",
        "\n",
        "        # debug\n",
        "        # print(\"initialized total reward\")\n",
        "\n",
        "    # gather reward\n",
        "    def gather_reward(self, reward):\n",
        "        self.total_reward += reward\n",
        "\n",
        "    # get total rewards\n",
        "    def get_total_reward(self):\n",
        "        return self.total_reward\n",
        "\n",
        "    # we start from large epsilon and gradually decay the epsilon in each episode\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "            \n",
        "            # debug\n",
        "            # print(\"decayed epsilon\")\n",
        "            \n",
        "    # reshape state for model\n",
        "    def reshape_state(self, state):\n",
        "        return np.reshape(state, newshape = (self.n_in, ))\n",
        "\n",
        "    # update q values\n",
        "    # train online model\n",
        "    def q_update(self):\n",
        "        # batch to update q values and train online model\n",
        "        batch = self.replay_buffer.read(self.batch_size)\n",
        "        \n",
        "        y_batch = []\n",
        "        \n",
        "        for b in batch:\n",
        "            \n",
        "            # debug\n",
        "            # print(\"working?\")\n",
        "            \n",
        "            last_observation, action, reward, observation = b\n",
        "            \n",
        "            last_observation = self.reshape_state(last_observation)\n",
        "            # [0] because tf nn output is list of list\n",
        "            q_last = self.online_model.predict(last_observation)[0]\n",
        "            \n",
        "            if observation is None:\n",
        "                # TODO 20191209 Yuki\n",
        "                q_this = reward\n",
        "            else:\n",
        "                observation = self.reshape_state(observation)\n",
        "                \n",
        "                # Double DQN logic ---------------------------------------------------------\n",
        "                # select action by online model\n",
        "                action_online = np.argmax(self.online_model.predict(observation))\n",
        "                # evaluate action by target model\n",
        "                q_this_target = self.target_model.predict(observation)[0][action_online]\n",
        "                # Bellman equation\n",
        "                q_this = reward + self.gamma * q_this_target\n",
        "                # --------------------------------------------------------------------------\n",
        "                \n",
        "            # update q values\n",
        "            q_last[action] = q_this\n",
        "            # store y data\n",
        "            y_batch.append(q_last)\n",
        "            \n",
        "        # numpy for tf nn model\n",
        "        # b[0] is last_observation\n",
        "        x_batch = np.array([b[0] for b in batch])\n",
        "        y_batch = np.array(y_batch)\n",
        "        \n",
        "        # train online model\n",
        "        history = self.online_model.fit(x_batch, y_batch, epochs = 1, verbose = 0)\n",
        "        \n",
        "        # debug\n",
        "        # print(\"tained online model\")\n",
        "        \n",
        "        # return online model loss\n",
        "        return history.history['loss'][0]\n",
        "        \n",
        "    # update target model\n",
        "    def update_target_model(self):\n",
        "        # get_weights returns list of weights of each layer\n",
        "        theta_online = self.online_model.get_weights()\n",
        "        theta_target = self.target_model.get_weights()\n",
        "        \n",
        "        # soft target update from \"continuous control with DRL\"\n",
        "        counter = 0\n",
        "        for weight_online, weight_target in zip(theta_online, theta_target):\n",
        "            # This equations need to be compared with paper\n",
        "            # target weight is a weighted average of target weight and online weight\n",
        "            weight_target = weight_target * (1 - self.tau) + weight_online * self.tau\n",
        "            # update target weight\n",
        "            theta_target[counter] = weight_target\n",
        "            # iterate\n",
        "            counter += 1\n",
        "        \n",
        "        # update target model\n",
        "        self.target_model.set_weights(theta_target)\n",
        "        \n",
        "        # debug\n",
        "        # print(\"updated target model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDQFiZpulOUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## experience replay\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self):\n",
        "        self.buffer = []\n",
        "        self.length = 0\n",
        "        self.max_length = 10000\n",
        "\n",
        "    def write(self, data):\n",
        "        if self.length >= self.max_length:\n",
        "            self.buffer.pop(0)\n",
        "            self.length -= 1\n",
        "        self.buffer.append(data)\n",
        "        self.length += 1\n",
        "\n",
        "    def read(self, batch_size):\n",
        "        # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
        "        return random.sample(self.buffer, min(batch_size, self.length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfYnLFdllOUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "c600a7f3-c5b8-4050-bc24-1eddd6d4e1a7"
      },
      "source": [
        "## training\n",
        "\n",
        "agent = Agent()\n",
        "ep_rewards = []\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "\n",
        "    # initialize\n",
        "    # env.reset() in taxi returns index of states out of 500\n",
        "    last_observation = env.reset()\n",
        "    agent.set_total_reward(0)\n",
        "\n",
        "    # iterations within an episode\n",
        "    for t in range(T_RANGE):\n",
        "\n",
        "        # draw action\n",
        "        action = agent.choose_action(last_observation)\n",
        "        # draw next state and reward\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # when taxi drop a passenger at destination, done = True\n",
        "        if done == True:\n",
        "            observation = None\n",
        "\n",
        "        # accumulate reward\n",
        "        agent.gather_reward(reward)\n",
        "        agent.gather_experience(last_observation, action, reward, observation)\n",
        "\n",
        "        # update q values\n",
        "        # train online model\n",
        "        loss = agent.q_update()\n",
        "        \n",
        "        # update target model\n",
        "        agent.update_target_model()\n",
        "        \n",
        "        # iterate\n",
        "        last_observation = observation\n",
        "\n",
        "        # goal\n",
        "        if done == True:\n",
        "            ep_rewards.append(agent.get_total_reward())\n",
        "            break\n",
        "\n",
        "    # store last loss of online model\n",
        "    losses.append(loss)\n",
        "\n",
        "    # In each episode we decay epsilon\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    # Monitor total reward during episodes\n",
        "    if ep % MONITOR_INTERVAL == 0:\n",
        "        print(\"episode:\", ep,\n",
        "              \"reward:\", agent.get_total_reward(),\n",
        "              \"loss:\", np.round(loss, decimals = 3), \n",
        "              \"epsilon:\", np.round(agent.epsilon, decimals = 5),\n",
        "              \"time: {} seconds\".format(np.round(time.time() - start_time, decimals = 0)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0 reward: -551 loss: 1.593 epsilon: 0.99995 time: 28.0 seconds\n",
            "episode: 1 reward: -560 loss: 0.884 epsilon: 0.9999 time: 56.0 seconds\n",
            "episode: 2 reward: -587 loss: 0.99 epsilon: 0.99985 time: 84.0 seconds\n",
            "episode: 3 reward: -560 loss: 1.052 epsilon: 0.9998 time: 112.0 seconds\n",
            "episode: 4 reward: -587 loss: 0.805 epsilon: 0.99975 time: 140.0 seconds\n",
            "episode: 5 reward: -506 loss: 1.851 epsilon: 0.9997 time: 168.0 seconds\n",
            "episode: 6 reward: -560 loss: 4.794 epsilon: 0.99965 time: 195.0 seconds\n",
            "episode: 7 reward: -542 loss: 6.277 epsilon: 0.9996 time: 223.0 seconds\n",
            "episode: 8 reward: -488 loss: 7.406 epsilon: 0.99955 time: 251.0 seconds\n",
            "episode: 9 reward: -515 loss: 7.368 epsilon: 0.9995 time: 279.0 seconds\n",
            "episode: 10 reward: -614 loss: 20.849 epsilon: 0.99945 time: 307.0 seconds\n",
            "episode: 11 reward: -587 loss: 36.804 epsilon: 0.9994 time: 336.0 seconds\n",
            "episode: 12 reward: -542 loss: 17.531 epsilon: 0.99935 time: 363.0 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-007cb7a3ecce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# update q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# train online model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# update target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-dd73dbaade46>\u001b[0m in \u001b[0;36mq_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mlast_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# [0] because tf nn output is list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mq_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3476\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m     output_structure = nest.pack_sequence_as(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m         \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqEEbaO0lOUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## debug q update\n",
        "\n",
        "# experiences = []\n",
        "# # (last_observation, action, reward, observation)\n",
        "# experiences.append((1, 0, -1, 2))\n",
        "# experiences.append((2, 1, -1, 3))\n",
        "# experiences.append((3, 2, -1, 4))\n",
        "# # print(experiences)\n",
        "\n",
        "# BATCH_SIZE = 2\n",
        "\n",
        "# net = Network(1, 6)\n",
        "# online_model = net._build_model()\n",
        "# target_model = net._build_model()\n",
        "\n",
        "# # reshape state for model\n",
        "# def reshape_state(state):\n",
        "#     return np.reshape(state, newshape = (STATE_SIZE, -1))\n",
        "\n",
        "\n",
        "# # def q_update(self):\n",
        "# def q_update():\n",
        "#     # sample batch\n",
        "#     batch = random.sample(experiences, BATCH_SIZE)\n",
        "#     print(batch)\n",
        "    \n",
        "#     # [0] because list of list\n",
        "#     last_state_input = reshape_state([s[0] for s in batch])[0]\n",
        "#     state_input = reshape_state([b[3] for b in batch if b[3] is not None])[0]\n",
        "#     print(last_state_input)\n",
        "#     print(state_input)\n",
        "    \n",
        "#     q_last = online_model.predict(last_state_input)\n",
        "#     print(q_last)\n",
        "    \n",
        "#     q_this = np.zeros_like(q_last)\n",
        "#     print(q_this)\n",
        "    \n",
        "#     ind_not_none = [i for i in range(np.shape(batch)[0]) if batch[i][3] is not None]\n",
        "#     print(ind_not_none)\n",
        "    \n",
        "#     action_online_model = np.argmax(online_model.predict(state_input), axis = 1)\n",
        "#     print(\"action_online_model\", action_online_model)\n",
        "#     # print(online_model.predict(state_input))\n",
        "    \n",
        "#     print(\"target_model.predict(state_input) \\n\", target_model.predict(state_input))\n",
        "#     pred_target_model = target_model.predict(state_input)\n",
        "#     # print(pred_target_model)\n",
        "#     # print([outputs for outputs in pred_target_model])\n",
        "    \n",
        "#     q_value_target_model = []\n",
        "#     for i in range(len(action_online_model)):\n",
        "#         tmp = pred_target_model[i][action_online_model[i]]\n",
        "#         q_value_target_model.append(tmp)\n",
        "#     print(q_value_target_model)\n",
        "    \n",
        "# q_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk0vWZjtlOUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # easier version of q update\n",
        "\n",
        "# experiences = []\n",
        "# random.seed(1)\n",
        "# # (last_observation, action, reward, observation)\n",
        "# experiences.append((1, 0, -1, 2))\n",
        "# experiences.append((2, 1, -1, 3))\n",
        "# experiences.append((3, 2, -1, 4))\n",
        "# # print(experiences)\n",
        "\n",
        "# BATCH_SIZE = 2\n",
        "\n",
        "# net = Network(1, 6)\n",
        "# online_model = net._build_model()\n",
        "# target_model = net._build_model()\n",
        "\n",
        "# # reshape state for model\n",
        "# def reshape_state(state):\n",
        "#     # return np.reshape(state, newshape = (STATE_SIZE, -1))\n",
        "#     return np.reshape(state, newshape = (STATE_SIZE, ))\n",
        "\n",
        "\n",
        "# # def q_update(self):\n",
        "# def q_update():\n",
        "#     # sample batch\n",
        "#     batch = random.sample(experiences, BATCH_SIZE)\n",
        "#     # print(batch)\n",
        "    \n",
        "#     minibatch_new_q_values = []\n",
        "    \n",
        "#     for experience in batch:\n",
        "#         last_state, action, reward, state = experience\n",
        "        \n",
        "#         last_state = reshape_state(last_state)\n",
        "#         # [0] for list of list\n",
        "#         experience_new_q_values = online_model.predict(last_state)[0]\n",
        "#         # print(experience_new_q_values)\n",
        "        \n",
        "#         if state is None:\n",
        "#             q_update = reward\n",
        "#         else:\n",
        "#             state = reshape_state(state)\n",
        "#             action_online = np.argmax(online_model.predict(state))\n",
        "#             # print(\"action_online\", action_online)\n",
        "#             # [0] for list of list\n",
        "#             q_value_target = target_model.predict(state)[0][action_online]\n",
        "#             # print(q_value_target)\n",
        "#             q_update = reward + GAMMA * q_value_target\n",
        "        \n",
        "#         # print(\"before\", experience_new_q_values)\n",
        "#         experience_new_q_values[action] = q_update\n",
        "#         # print(\"after\", experience_new_q_values)\n",
        "#         minibatch_new_q_values.append(experience_new_q_values)\n",
        "#         # print(minibatch_new_q_values)\n",
        "        \n",
        "#     x_batch = np.array([b[0] for b in batch])\n",
        "#     print(x_batch)\n",
        "#     # x_batch = reshape_state(x_batch)\n",
        "#     # print(x_batch)\n",
        "#     y_batch = np.array(minibatch_new_q_values)\n",
        "#     print(y_batch)\n",
        "    \n",
        "#     online_model.fit(x_batch, y_batch, epochs = 1)\n",
        "    \n",
        "# q_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB4EHF0WlOUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## debug neural network model\n",
        "\n",
        "# net = Network(1, 6)\n",
        "# model = net._build_model()\n",
        "# model.summary()\n",
        "\n",
        "# data = 1\n",
        "# tmp = np.reshape(data, newshape = (STATE_SIZE, -1))\n",
        "# print(tmp.shape)\n",
        "# print(model.predict(tmp))\n",
        "\n",
        "# tmp = model.get_weights()\n",
        "# # print(type(tmp))\n",
        "# # print(len(tmp))\n",
        "# # print(tmp[0])\n",
        "# # print(len(tmp[0][0]))\n",
        "# # print(tmp[1])\n",
        "# print(tmp[0].shape)\n",
        "# print(tmp[1].shape)\n",
        "# print(tmp[2].shape)\n",
        "# print(tmp[3].shape)\n",
        "# print(tmp[4].shape)\n",
        "# print(tmp[5].shape)\n",
        "# # print(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}