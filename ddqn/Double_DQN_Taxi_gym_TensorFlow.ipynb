{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN (DDQN) Taxi with gym and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameter\n",
    "\n",
    "ENV_NAME = 'Taxi-v3'\n",
    "\n",
    "# seed for initial position in environment\n",
    "SEED = 123\n",
    "\n",
    "# the learning rate used by RMSProp in \"human-level control through deep reinforcement learning\"\n",
    "LEARNING_RATE = 0.00025\n",
    "MOMENTUM = 0.95\n",
    "\n",
    "# taxi environment\n",
    "STATE_SIZE = 1\n",
    "ACTION_SIZE = 6\n",
    "\n",
    "# soft target update\n",
    "# value used in \"continuous control with deep reinforcement learning\"\n",
    "TAU = 0.001 \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# gamma in Bellman equation\n",
    "GAMMA = 0.99\n",
    "\n",
    "# epsilon in epsilon greedy algorithm\n",
    "# we implement epsilon decay\n",
    "EPSILON = 1.0\n",
    "# EPSILON_DECAY = 0.99999\n",
    "EPSILON_DECAY = 0.99995\n",
    "# EPSILON_MIN = 0.1\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "# max step in each episode\n",
    "T_RANGE = 201\n",
    "\n",
    "# taxi environment\n",
    "STATE_SIZE = 1\n",
    "ACTION_SIZE = 6\n",
    "\n",
    "# training\n",
    "EPISODES = 5000\n",
    "MONITOR_INTERVAL = 100\n",
    "# EPISODES = 100\n",
    "# MONITOR_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## environment\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "# env.seed(SEED)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experience replay\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.length = 0\n",
    "        self.max_length = 10000\n",
    "\n",
    "    def write(self, data):\n",
    "        if self.length >= self.max_length:\n",
    "            self.buffer.pop(0)\n",
    "            self.length -= 1\n",
    "        self.buffer.append(data)\n",
    "        self.length += 1\n",
    "\n",
    "    def read(self, batch_size):\n",
    "        # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
    "        return random.sample(self.buffer, min(batch_size, self.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## network\n",
    "\n",
    "# we use the same architectures for online_network and target_network\n",
    "# we _build_model 2 times\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape = (self.n_in,), activation = 'relu'))\n",
    "        model.add(Dense(48, activation = 'relu'))\n",
    "        model.add(Dense(self.n_out, activation = 'linear'))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(LEARNING_RATE)\n",
    "        model.compile(loss = 'mse', optimizer = optimizer)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## agent\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.n_in = STATE_SIZE\n",
    "        self.n_out = ACTION_SIZE\n",
    "        self.total_reward = 0\n",
    "        self.gamma = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.replay_buffer = Replay()\n",
    "        self.online_model = Network(self.n_in, self.n_out)._build_model()\n",
    "        self.target_model = Network(self.n_in, self.n_out)._build_model()\n",
    "\n",
    "    def gather_experience(self, last_observation, action, reward, observation):\n",
    "        self.replay_buffer.write((last_observation, action, reward, observation))\n",
    "\n",
    "    # return action index\n",
    "    def choose_action(self, observation):\n",
    "        # epsilon greedy policy is performed here\n",
    "        # exploitation\n",
    "        # np.random.rand is uniform [0,1]\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.argmax(self.model.predict(np.array([observation])))\n",
    "        # exploration\n",
    "        else:\n",
    "            # random action from 0 to 5 out of 6 actions\n",
    "            return int(np.random.randint(low = 0, high = ACTION_SIZE-1, size = 1, dtype = 'int'))\n",
    "\n",
    "    # set total reward\n",
    "    def set_total_reward(self, new_total):\n",
    "        self.total_reward = new_total\n",
    "\n",
    "    # gather reward\n",
    "    def gather_reward(self, reward):\n",
    "        self.total_reward += reward\n",
    "\n",
    "    # get total rewards\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    # we start from large epsilon and gradually decay the epsilon in each episode\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    # reshape state for model\n",
    "    def reshape_state(self, state):\n",
    "        return np.reshape(state, newshape = (self.n_in, -1))\n",
    "\n",
    "    # q update\n",
    "    # also contains online network update\n",
    "    def q_update(self):\n",
    "        # get a batch from replay buffer\n",
    "        # batch is list of turples\n",
    "        batch = self.replay_buffer.read(self.batch_size)\n",
    "        \n",
    "        # reshape state for model input\n",
    "        # batch[i][0], or s[0] is last_observation (first element of turple in the list is state index)\n",
    "        # batch[i][3], or b[3] is observation\n",
    "        last_state_input = reshape_state([s[0] for s in batch])\n",
    "        state = reshape_state([b[3] for b in batch if b[3] is not None])\n",
    "        \n",
    "        # experience new q values ([0] because list of list)\n",
    "        q_last = self.online_model.predict(last_state_input)[0]\n",
    "\n",
    "        # initialize\n",
    "        q_this = np.zeros_like(q_last)\n",
    "        \n",
    "        # batch[i][3] is observation\n",
    "        ind_not_none = [i for i in range(np.shape(batch)[0]) if batch[i][3] is not None]\n",
    "        \n",
    "        # select action by online model\n",
    "        action_online_model = np.argmax(self.online_model.predict(state))\n",
    "        \n",
    "        # Bellman equation of Double DQN\n",
    "        # evaluate action by target model\n",
    "        q_value_target_model = self.target_model.predict(state)[action_online_model]\n",
    "        \n",
    "        \n",
    "        q_value = \n",
    "    \n",
    "        for i in range(len(ind_not_none)):\n",
    "            # store n_out number of q predictions by neural network regression\n",
    "            q_this[ind_not_none[i], :] = q_this_not_none[i, :]\n",
    "\n",
    "        # initialize batch for online model training data\n",
    "        x_batch = np.zeros([np.shape(batch)[0], self.n_in])\n",
    "        y_batch = np.zeros([np.shape(batch)[0], self.n_out])\n",
    "\n",
    "        # x batch for online model\n",
    "        for i in range(np.shape(batch)[0]):\n",
    "            # batch[i][0] is last_observation\n",
    "            x_batch[i, :] = batch[i][0]\n",
    "\n",
    "        # y batch for online model\n",
    "        for j in range(self.n_out):\n",
    "            # batch[i][1] is action\n",
    "            if j == batch[i][1]:\n",
    "                \n",
    "                # Bellman equation\n",
    "                # Q values update of Double DQN\n",
    "                # batch[i][2] is reward\n",
    "                y_batch[i, j] = batch[i][2] + self.gamma * q_value_target_model\n",
    "                \n",
    "            else:\n",
    "                y_batch[i, j] = q_last[i][j]\n",
    "\n",
    "        # train online model\n",
    "        self.online_model.fit(x_batch, y_batch, epochs = 1, verbose = 0)\n",
    "        \n",
    "    # update target model\n",
    "    def update_target_model(self):\n",
    "        # get_weights returns list of weights of each layer\n",
    "        theta_online = self.online_model.get_weights()\n",
    "        theta_target = self.target_model.get_weights()\n",
    "        \n",
    "        # soft target update from \"continuous control with DRL\"\n",
    "        counter = 0\n",
    "        for weight_online, weight_target in zip(theta_online, theta_target):\n",
    "            # This equations need to be compared with paper\n",
    "            # target weight is a weighted average of target weight and online weight\n",
    "            weight_target = weight_target * (1 - self.tau) + weight_online * self.tau\n",
    "            # update target weight\n",
    "            theta_target[counter] = weight_target\n",
    "            # iterate\n",
    "            counter += 1\n",
    "        \n",
    "        # update target model\n",
    "        self.target_model.set_weights(theta_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experience replay\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.length = 0\n",
    "        self.max_length = 10000\n",
    "\n",
    "    def write(self, data):\n",
    "        if self.length >= self.max_length:\n",
    "            self.buffer.pop(0)\n",
    "            self.length -= 1\n",
    "        self.buffer.append(data)\n",
    "        self.length += 1\n",
    "\n",
    "    def read(self, batch_size):\n",
    "        # at beginning buffer is almost empty, so batch is smaller than batch_size\n",
    "        return random.sample(self.buffer, min(batch_size, self.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "\n",
    "agent = Agent()\n",
    "ep_rewards = []\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "\n",
    "  # monitor training process\n",
    "  if ep % MONITOR_INTERVAL == 0:\n",
    "    print(\"episode\", ep, \"epsilon\", agent.epsilon)\n",
    "\n",
    "  # initialize\n",
    "  # env.reset() in taxi returns index of states out of 500\n",
    "  last_observation = env.reset()\n",
    "  agent.set_total_reward(0)\n",
    "\n",
    "  # iterations within an episode\n",
    "  for t in range(T_RANGE):\n",
    "\n",
    "    # draw action\n",
    "    action = agent.choose_action(last_observation)\n",
    "    # draw next state and reward\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # when taxi drop a passenger at destination, done = True\n",
    "    if done == True:\n",
    "      observation = None\n",
    "\n",
    "    # accumulate reward\n",
    "    agent.gather_reward(reward)\n",
    "    agent.gather_experience(last_observation, action, reward, observation)\n",
    "\n",
    "    # update q function\n",
    "    agent.q_update()\n",
    "    # iterate\n",
    "    last_observation = observation\n",
    "        \n",
    "    # goal\n",
    "    if done == True:\n",
    "      ep_rewards.append(agent.get_total_reward())\n",
    "      break\n",
    "\n",
    "  # In each episode we decay epsilon\n",
    "  agent.decay_epsilon()\n",
    "\n",
    "  # Monitor total reward during episodes\n",
    "  if ep % MONITOR_INTERVAL == 0:\n",
    "    print(\"reward\", agent.get_total_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0, -1, 2), (3, 2, -1, 4)]\n",
      "[1 3]\n",
      "[2 4]\n",
      "[[ 0.01578611  0.32744968 -0.09944885  0.09646658  0.05530835  0.03442656]\n",
      " [ 0.04735828  0.9823491  -0.29834646  0.28939974  0.16592506  0.10327965]]\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "[0, 1]\n",
      "action_online_model [1 1]\n",
      "target_model.predict(state_input) \n",
      " [[-0.15322496  0.01192236  0.154649    0.6328393   0.197792    0.080158  ]\n",
      " [-0.30644992  0.02384472  0.309298    1.2656786   0.395584    0.160316  ]]\n",
      "[0.0119223595, 0.023844719]\n"
     ]
    }
   ],
   "source": [
    "## debug q update\n",
    "\n",
    "experiences = []\n",
    "# (last_observation, action, reward, observation)\n",
    "experiences.append((1, 0, -1, 2))\n",
    "experiences.append((2, 1, -1, 3))\n",
    "experiences.append((3, 2, -1, 4))\n",
    "# print(experiences)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "net = Network(1, 6)\n",
    "online_model = net._build_model()\n",
    "target_model = net._build_model()\n",
    "\n",
    "# reshape state for model\n",
    "def reshape_state(state):\n",
    "    return np.reshape(state, newshape = (STATE_SIZE, -1))\n",
    "\n",
    "\n",
    "# def q_update(self):\n",
    "def q_update():\n",
    "    # sample batch\n",
    "    batch = random.sample(experiences, BATCH_SIZE)\n",
    "    print(batch)\n",
    "    \n",
    "    # [0] because list of list\n",
    "    last_state_input = reshape_state([s[0] for s in batch])[0]\n",
    "    state_input = reshape_state([b[3] for b in batch if b[3] is not None])[0]\n",
    "    print(last_state_input)\n",
    "    print(state_input)\n",
    "    \n",
    "    q_last = online_model.predict(last_state_input)\n",
    "    print(q_last)\n",
    "    \n",
    "    q_this = np.zeros_like(q_last)\n",
    "    print(q_this)\n",
    "    \n",
    "    ind_not_none = [i for i in range(np.shape(batch)[0]) if batch[i][3] is not None]\n",
    "    print(ind_not_none)\n",
    "    \n",
    "    action_online_model = np.argmax(online_model.predict(state_input), axis = 1)\n",
    "    print(\"action_online_model\", action_online_model)\n",
    "    # print(online_model.predict(state_input))\n",
    "    \n",
    "    print(\"target_model.predict(state_input) \\n\", target_model.predict(state_input))\n",
    "    pred_target_model = target_model.predict(state_input)\n",
    "    # print(pred_target_model)\n",
    "    # print([outputs for outputs in pred_target_model])\n",
    "    \n",
    "    q_value_target_model = []\n",
    "    for i in range(len(action_online_model)):\n",
    "        tmp = pred_target_model[i][action_online_model[i]]\n",
    "        q_value_target_model.append(tmp)\n",
    "    print(q_value_target_model)\n",
    "    \n",
    "q_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## debug neural network model\n",
    "\n",
    "# net = Network(1, 6)\n",
    "# model = net._build_model()\n",
    "# model.summary()\n",
    "\n",
    "# data = 1\n",
    "# tmp = np.reshape(data, newshape = (STATE_SIZE, -1))\n",
    "# print(tmp.shape)\n",
    "# print(model.predict(tmp))\n",
    "\n",
    "# tmp = model.get_weights()\n",
    "# # print(type(tmp))\n",
    "# # print(len(tmp))\n",
    "# # print(tmp[0])\n",
    "# # print(len(tmp[0][0]))\n",
    "# # print(tmp[1])\n",
    "# print(tmp[0].shape)\n",
    "# print(tmp[1].shape)\n",
    "# print(tmp[2].shape)\n",
    "# print(tmp[3].shape)\n",
    "# print(tmp[4].shape)\n",
    "# print(tmp[5].shape)\n",
    "# # print(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
